{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function    # (at top of module)\n",
    "import warnings\n",
    "#warnings.filterwarnings('always')\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import json\n",
    "import spotipy\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "import seaborn as sns\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the file\n",
    "data = pd.read_csv('Data/data_500_entries_youtube.csv')\n",
    "print(\"Number of entries in original data: \" + str(len(data.index)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'song_id' in data.columns:\n",
    "    data = data.drop_duplicates(subset=['song_id'], keep='first')\n",
    "else:\n",
    "    data = data.drop_duplicates(subset=['song_title'], keep='first')\n",
    "    \n",
    "print(\"Number of entries in original data after cleaning: \" + str(len(data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_modules import *\n",
    "# Make a copy of the data to which we will ad labels and then remove any \n",
    "# columns that we will not need\n",
    "# This is currently a duplicate of the functionality above - could maybe only do this in one place\n",
    "\n",
    "final_data = label_data(data, 89)\n",
    "\n",
    "# Drop unnecessary columns from original data\n",
    "if 'total_no_streams' in data.columns:\n",
    "    final_data.drop(['song_id', 'song_title', 'artist', 'popularity', 'total_no_streams', 'youtube_view_count', 'youtube_video_title'], 1, inplace=True)\n",
    "else:\n",
    "    final_data.drop(['song_id','song_title', 'artist', 'popularity', 'youtube_view_count', 'youtube_video_title'], 1, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X will be our examples and y will be our labels\n",
    "X = final_data.drop('is_popular', axis=1)\n",
    "y = final_data['is_popular']\n",
    "X_fs, X_test, y_fs, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "# Sanity checks\n",
    "print(\"Number of entries in actual data: \" + str(len(X.index)))\n",
    "print(\"Number of entries in label data: \" + str(len(y.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "\n",
    "COLUMNS_TO_SCALE = [\"energy\", \"liveness\", \"tempo\", \n",
    "                    \"speechiness\", \"acousticness\", \"instrumentalness\", \n",
    "                    \"time_signature\", \"danceability\", \"key\", \n",
    "                    \"duration\", \"loudness\", \"valence\", \"mode\"]\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_fs)\n",
    "\n",
    "# Copy data back\n",
    "X_fs = pd.DataFrame(scaler.transform(X_fs), columns=COLUMNS_TO_SCALE)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=COLUMNS_TO_SCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_fs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating all the different possible feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "columns = [\"energy\", \"liveness\", \"tempo\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"time_signature\", \"danceability\",\n",
    "          \"key\", \"duration\", \"loudness\", \"valence\", \"mode\"]\n",
    "allSubsets = []\n",
    "total_no_subsets = 0\n",
    "for m in range(1, len(columns) + 1, 1):\n",
    "    subsets_with_m_elem = list(itertools.combinations(columns, m))\n",
    "    total_no_subsets += len(subsets_with_m_elem)\n",
    "    allSubsets.append(subsets_with_m_elem)\n",
    "print(total_no_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(fs_scores):\n",
    "\n",
    "    # Print feature sets with best scores\n",
    "    for cur_list in fs_scores:\n",
    "        print(cur_list[0])\n",
    "        print()\n",
    "        \n",
    "        max_score_gen = 0\n",
    "        max_score_cv = 0\n",
    "        score_cv = 0\n",
    "        score_gen = 0\n",
    "        for i in range(0, len(cur_list[1]), 1):\n",
    "            cur_score_gen = cur_list[1][i][1]\n",
    "            cur_score_cv = cur_list[1][i][2]\n",
    "\n",
    "            if cur_score_gen > max_score_gen:\n",
    "                max_score_gen = cur_score_gen\n",
    "                score_cv = cur_score_cv\n",
    "                sel_feature_set_gen = cur_list[1][i][0]\n",
    "\n",
    "            if cur_score_cv > max_score_cv:\n",
    "                max_score_cv = cur_score_cv\n",
    "                score_gen = cur_score_gen\n",
    "                sel_feature_set_cv = cur_list[1][i][0]\n",
    "        \n",
    "        print(\"Max generalized score: \")\n",
    "        print(max_score_gen)\n",
    "        print(\"Average cv score for that feature subset: \")\n",
    "        print(score_cv)\n",
    "        print()\n",
    "        print(sel_feature_set_gen)\n",
    "        print(\"-------------------\")\n",
    "        print(\"Max average cv score: \")\n",
    "        print(max_score_cv)\n",
    "        print(\"Generalized score for that feature subset: \")\n",
    "        print(score_gen)\n",
    "        print()\n",
    "        print(sel_feature_set_cv)\n",
    "        print()\n",
    "        print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing same size feature subsets against eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "models = [\n",
    "          {'title':\"Logistic regression\", 'model':LogisticRegression(random_state=3)},\n",
    "          {'title':\"Logistic regression balanced weights\", 'model':LogisticRegression(class_weight='balanced', random_state=3)},\n",
    "          {'title':\"Oversampling logistic regression\", 'model':make_pipeline_imb(SMOTE(random_state=4), LogisticRegression(random_state=3))},\n",
    "          {'title':\"Oversampling logistic regression balanced weights\", 'model':make_pipeline_imb(SMOTE(random_state=4), LogisticRegression(class_weight='balanced',random_state=3))},\n",
    "          {'title':\"KNN\", 'model':KNeighborsClassifier(n_neighbors = 17)},\n",
    "          {'title':\"Oversampling KNN\", 'model':make_pipeline_imb(SMOTE(random_state=4), KNeighborsClassifier(n_neighbors = 17))},\n",
    "          {'title':\"SVM\", 'model':svm.SVC(probability=True, gamma='scale', random_state=3)},\n",
    "          {'title':\"SVM balanced weights\", 'model':svm.SVC(probability=True, gamma='scale', random_state=3, class_weight='balanced')},\n",
    "          {'title':\"Oversampling SVM\", 'model':make_pipeline_imb(SMOTE(random_state=4), svm.SVC(probability=True, gamma='scale', random_state=3))}\n",
    "         ]\n",
    "models_scores_list = []\n",
    "\n",
    "X_fs, X_test, y_fs, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "for item in models:\n",
    "    steps_taken = 0\n",
    "    scores_list = []\n",
    "    # Loop thorugh all the different subsets of 1,2,3, ..., (#features) elements\n",
    "    for n_elem_subsets in range(0, len(columns), 1):\n",
    "        len_elem_subs = len(allSubsets[n_elem_subsets])\n",
    "        best_score = 0\n",
    "        best_feature_set = []\n",
    "        \n",
    "        # Loop through all the subsets of n_elem_subsets elements\n",
    "        for cur_comb in range(0, len_elem_subs, 1):\n",
    "\n",
    "            # Current feature set\n",
    "            cur_feature_set = list(allSubsets[n_elem_subsets][cur_comb])\n",
    "\n",
    "            # Get the data with the current subset of features\n",
    "            cur_data = X_fs[cur_feature_set]\n",
    "\n",
    "            # Transform to np array for faster computation\n",
    "            cur_data_np = np.array(cur_data)\n",
    "            cur_labels_np = np.array(y_fs)\n",
    "            \n",
    "            # Counter of number of iterations\n",
    "            steps_taken += 1\n",
    "\n",
    "            # Instantiate model\n",
    "            model = item['model']\n",
    "            # model = svm.SVC(probability=True, gamma='scale', random_state=3, class_weight='balanced')\n",
    "\n",
    "            # Instantiate cross validation strategy\n",
    "            cv = StratifiedKFold(n_splits=10, random_state=3)\n",
    "\n",
    "            scores = np.array([])\n",
    "            for train, test in cv.split(cur_data_np, cur_labels_np):\n",
    "                probas_ = model.fit(cur_data_np[train], cur_labels_np[train]).predict_proba(cur_data_np[test])\n",
    "                predicts = model.predict(cur_data_np[test])\n",
    "                scores = np.append(scores, roc_auc_score(cur_labels_np[test], probas_[:, 1]))\n",
    "\n",
    "            mean_score = scores.mean()\n",
    "\n",
    "            # average_score = (float(mean_score) + test_after_auc)/2\n",
    "            # print(\"Curent score for {} features : {} | Test after score: {} | Average: {}\".format(len(cur_feature_set), mean_score, test_after_auc, average_score))\n",
    "\n",
    "            # For brevity, we only keep the best feature subset of one size\n",
    "            # Alternatively, we could keep the scores for all the different combinations\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_feature_set = list(allSubsets[n_elem_subsets][cur_comb])\n",
    "\n",
    "        #predictions = model.predict(X_test[feature_set])\n",
    "        model = item['model']\n",
    "        model.fit(X_fs[best_feature_set], y_fs)\n",
    "        y_pred_prob = model.predict_proba(X_test[best_feature_set])[:,1]\n",
    "        test_after_auc = roc_auc_score(y_test, y_pred_prob)   \n",
    "        #Keep the best score in a list    \n",
    "        scores_list.append([best_feature_set, test_after_auc])\n",
    "\n",
    "    print(\"Number of iterations: \", steps_taken)\n",
    "    models_scores_list.append([item['title'], scores_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring looks like: \n",
    "[Model Name, [feature subset, generalize score, average cv score]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(models_scores_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing all subsets against eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models = [\n",
    "#          {'title':\"Logistic regression\", 'model':LogisticRegression(random_state=3), 'feature_set':['liveness', 'tempo', 'acousticness', 'instrumentalness', 'time_signature', 'danceability', 'key', 'loudness', 'valence']},\n",
    "#           {'title':\"Logistic regression balanced weights\", 'model':LogisticRegression(class_weight='balanced', n_jobs=-1, solver=\"lbfgs\"), 'feature_set':['tempo', 'acousticness', 'danceability', 'valence']},\n",
    "#          {'title':\"Oversampling logistic regression\", 'model':make_pipeline_imb(SMOTE(random_state=4), LogisticRegression(random_state=3, n_jobs=-1)), 'feature_set':['tempo', 'acousticness', 'danceability', 'key', 'valence']},\n",
    "#          {'title':\"Oversampling logistic regression balanced weights\", 'model':make_pipeline_imb(SMOTE(random_state=4), LogisticRegression(class_weight='balanced',random_state=3)), 'feature_set':['tempo', 'acousticness', 'danceability', 'key', 'valence']},\n",
    "#          {'title':\"KNN\", 'model':KNeighborsClassifier(n_neighbors = 17), 'feature_set':['liveness', 'tempo', 'acousticness', 'danceability']},\n",
    "#          {'title':\"Oversampling KNN\", 'model':make_pipeline_imb(SMOTE(random_state=4), KNeighborsClassifier(n_neighbors = 17)), 'feature_set':['energy', 'instrumentalness', 'duration', 'valence', 'mode']},\n",
    "#          {'title':\"SVM\", 'model':svm.SVC(probability=True, gamma='scale', random_state=3), 'feature_set':['liveness', 'tempo', 'acousticness', 'danceability', 'loudness', 'mode']},\n",
    "#           {'title':\"SVM balanced weights\", 'model':svm.SVC(probability=True, gamma='scale',class_weight='balanced', kernel='rbf'), 'feature_set':['speechiness', 'key']},\n",
    "#          {'title':\"Oversampling SVM\", 'model':make_pipeline_imb(SMOTE(random_state=4), svm.SVC(probability=True, gamma='scale', random_state=3, kernel='rbf')), 'feature_set':['tempo', 'time_signature']},\n",
    "#           {'title':\"Multilayer Perceptron\", 'model':MLPClassifier(solver='lbfgs', alpha=1e-2), 'feature_set':['energy', 'acousticness', 'danceability', 'duration', 'valence', 'mode']},\n",
    "#          {'title':\"Multilayer Perceptron\", 'model':MLPClassifier(), 'feature_set':['energy', 'acousticness', 'danceability', 'duration', 'valence', 'mode']},\n",
    "          {'title':\"Oversampling Multilayer Perceptron\", 'model':make_pipeline_imb(SMOTE(random_state=4), MLPClassifier(solver=\"lbfgs\", activation=\"relu\", alpha=1, learning_rate=\"constant\")), 'feature_set':['loudness', 'speechiness']},\n",
    "          {'title':\"SGD Multilayer Perceptron\", 'model':make_pipeline_imb(SMOTE(random_state=4), MLPClassifier(activation = 'relu', solver='sgd', alpha=0.0001, learning_rate=\"constant\")), 'feature_set':['loudness', 'speechiness']},\n",
    "#          {'title':\"Random Forest Classifier balanced weights\", 'model':RandomForestClassifier(n_estimators=100, max_depth=2, random_state=3, class_weight=\"balanced\", n_jobs=-1), 'feature_set':['loudness', 'speechiness']}\n",
    "         ]\n",
    "models_scores_list = []\n",
    "\n",
    "for item in models:\n",
    "    steps_taken = 0\n",
    "    scores_list = []\n",
    "    # Loop thorugh all the different subsets of 1,2,3, ..., (#features) elements\n",
    "    for n_elem_subsets in range(0, len(columns), 1):\n",
    "        len_elem_subs = len(allSubsets[n_elem_subsets])\n",
    "        \n",
    "        # Loop through all the subsets of n_elem_subsets elements\n",
    "        for cur_comb in range(0, len_elem_subs, 1):\n",
    "\n",
    "            # Current feature set\n",
    "            cur_feature_set = list(allSubsets[n_elem_subsets][cur_comb])\n",
    "\n",
    "            # Get the data with the current subset of features\n",
    "            cur_data = X_fs[cur_feature_set]\n",
    "\n",
    "            # Transform to np array for faster computation\n",
    "            cur_data_np = np.array(cur_data)\n",
    "            cur_labels_np = np.array(y_fs)\n",
    "            \n",
    "            # Counter of number of iterations\n",
    "            steps_taken += 1\n",
    "            if steps_taken % 1000 == 0:\n",
    "                print(steps_taken)\n",
    "\n",
    "            # Instantiate model\n",
    "            model = item['model']\n",
    "\n",
    "            # Instantiate cross validation strategy\n",
    "            cv = StratifiedShuffleSplit(n_splits=10, random_state=5)\n",
    "\n",
    "            scores = np.array([])\n",
    "            \n",
    "            # Do CV for this current feature set\n",
    "            for train, test in cv.split(cur_data_np, cur_labels_np):\n",
    "                probas_ = model.fit(cur_data_np[train], cur_labels_np[train]).predict_proba(cur_data_np[test])\n",
    "                predicts = model.predict(cur_data_np[test])\n",
    "                scores = np.append(scores, roc_auc_score(cur_labels_np[test], probas_[:, 1]))\n",
    "            \n",
    "            # Average CV scores\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            # Calculate generalization score\n",
    "            model.fit(cur_data_np, cur_labels_np)\n",
    "            y_pred_prob = model.predict_proba(X_test[cur_feature_set])[:,1]\n",
    "            test_after_auc = roc_auc_score(y_test, y_pred_prob)   \n",
    "            #Keep the best score in a list    \n",
    "            scores_list.append([cur_feature_set, test_after_auc, mean_score])\n",
    "\n",
    "    print(\"Number of iterations: \", steps_taken)\n",
    "    models_scores_list.append([item['title'], scores_list])\n",
    "    \n",
    "    max_score_gen = 0\n",
    "    max_score_cv = 0\n",
    "    score_cv = 0\n",
    "    score_gen = 0\n",
    "    sel_feature_set_gen = []\n",
    "    sel_feature_set_cv = []\n",
    "    for i in range(0, len(scores_list), 1):\n",
    "        cur_score_gen = scores_list[i][1]\n",
    "        cur_score_cv = scores_list[i][2]\n",
    "        if cur_score_gen > max_score_gen:\n",
    "            max_score_gen = cur_score_gen\n",
    "            score_cv = cur_score_cv\n",
    "            sel_feature_set_gen = scores_list[i][0]\n",
    "            \n",
    "        if cur_score_cv > max_score_cv:\n",
    "            max_score_cv = cur_score_cv\n",
    "            score_gen = cur_score_gen\n",
    "            sel_feature_set_cv = scores_list[i][0]\n",
    "    print(item['title'])\n",
    "    print()\n",
    "    print(\"Max generalized score: \")\n",
    "    print(max_score_gen)\n",
    "    print(\"Average cv score for that feature subset: \")\n",
    "    print(score_cv)\n",
    "    print()\n",
    "    print(sel_feature_set_gen)\n",
    "    print(\"-------------------\")\n",
    "    print(\"Max average cv score: \")\n",
    "    print(max_score_cv)\n",
    "    print(\"Generalized score for that feature subset: \")\n",
    "    print(score_gen)\n",
    "    print()\n",
    "    print(sel_feature_set_cv)\n",
    "    print()\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring looks like: \n",
    "[Model Name, [feature subset, generalize score, average cv score]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5)\n",
    "\n",
    "sel = SelectKBest(chi2, k = 5)\n",
    "sel.fit(X_train, y_train)\n",
    "\n",
    "selected = sel.transform(X_train)\n",
    "#print(selected.shape)\n",
    "#print(X_train.shape)\n",
    "\n",
    "mask = sel.get_support() #list of booleans\n",
    "new_features = [] # The list of your K best features\n",
    "\n",
    "for bool, feature in zip(mask, columns):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(pearsonr(X['valence'], y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "feature_scores = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "for score, fname in sorted(zip(feature_scores, columns)):\n",
    "    print(fname, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"energy\", \"liveness\", \"tempo\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"time_signature\", \"danceability\",\n",
    "          \"key\", \"duration\", \"loudness\", \"valence\", \"mode\", 'youtube_view_count']\n",
    "jmi = 0\n",
    "for col in columns:\n",
    "    for next_col in columns:\n",
    "        if col != next_col:\n",
    "            jmi += mutual_info_classif(X_train[[col, next_col]], y_train)[0]\n",
    "            #print(col, next_col)\n",
    "    print(\"JMI for {} is {}\".format(col, jmi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "feature_scores = f_classif(X_train, y_train)[0]\n",
    "\n",
    "for score, fname in sorted(zip(feature_scores, columns)):\n",
    "    print(fname, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "lasso = LassoCV(cv=StratifiedKFold(n_splits=10, random_state=3), random_state=3)\n",
    "lasso.fit(X_train,y_train)\n",
    "train_score=lasso.score(X_train,y_train)\n",
    "test_score=lasso.score(X_test,y_test)\n",
    "coeff_used = np.sum(lasso.coef_!=0)\n",
    "\n",
    "print(\"training score:\", train_score)\n",
    "print(\"test score: \", test_score)\n",
    "print(\"number of features used: \", coeff_used)\n",
    "\n",
    "#print(lasso.coef_)\n",
    "\n",
    "ind = 0\n",
    "sel_f = []\n",
    "for coef in lasso.coef_:\n",
    "    if coef != 0:\n",
    "        sel_f.append(columns[ind]) \n",
    "    ind += 1\n",
    "print(\"Alpha(amount of penalization) chosen by CV: \", lasso.alpha_)\n",
    "print(sel_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "loglasso = LogisticRegression(penalty='l1', class_weight='balanced', random_state=3)\n",
    "loglasso.fit(X_train,y_train)\n",
    "train_score=loglasso.score(X_train,y_train)\n",
    "test_score=loglasso.score(X_test,y_test)\n",
    "coeff_used = np.sum(loglasso.coef_!=0)\n",
    "\n",
    "print(\"training score:\", train_score)\n",
    "print(\"test score: \", test_score)\n",
    "print(\"number of features used: \", coeff_used)\n",
    "\n",
    "print(loglasso.coef_)\n",
    "\n",
    "ind = 0\n",
    "sel_f = []\n",
    "for coef in loglasso.coef_[0]:\n",
    "    if coef != 0:\n",
    "        sel_f.append(columns[ind]) \n",
    "    ind += 1\n",
    "print(sel_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
