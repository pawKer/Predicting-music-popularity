{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function    # (at top of module)\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import json\n",
    "import spotipy\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "import seaborn as sns\n",
    "import io, os, sys, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the file\n",
    "data = pd.read_csv('Data/data_500_entries_youtube.csv')\n",
    "print(\"Number of entries in original data: \" + str(len(data.index)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'song_id' in data.columns:\n",
    "    data = data.drop_duplicates(subset=['song_id'], keep='first')\n",
    "else:\n",
    "    data = data.drop_duplicates(subset=['song_title'], keep='first')\n",
    "    \n",
    "print(\"Number of entries in original data after cleaning: \" + str(len(data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.popularity > 50]\n",
    "print(\"Number of entries in original data after cleaning: \" + str(len(data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_modules import *\n",
    "\n",
    "final_data = label_data(data, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns from original data - also some legacy support for a different data format\n",
    "if 'song_id' in data.columns:\n",
    "    final_data.drop(['song_id', 'song_title', 'artist', 'popularity', 'youtube_video_title', 'youtube_view_count'], 1, inplace=True)\n",
    "else:\n",
    "    final_data.drop(['song_title', 'artist', 'popularity'], 1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_SCALE = [\"energy\", \"liveness\", \"tempo\", \n",
    "                    \"speechiness\", \"acousticness\", \"instrumentalness\", \n",
    "                    \"time_signature\", \"danceability\", \"key\", \n",
    "                    \"duration\", \"loudness\", \"valence\", \"mode\"]\n",
    "#COLUMNS_TO_SCALE = [\"energy\", \"tempo\",\"loudness\", \"valence\"]\n",
    "\n",
    "# Keep data in a temp variable for testing\n",
    "scaled_data = scale_data_standardize(final_data, COLUMNS_TO_SCALE)\n",
    "\n",
    "# Plots to see the difference before/after scaling\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.plot(data['duration'])\n",
    "# plt.title(\"Duration before scaling\")\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(scaled_data['duration'])\n",
    "# plt.title(\"Duration after scaling\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Copy data back\n",
    "final_data = scaled_data.copy()\n",
    "\n",
    "#Just to check that everything is fine\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will be our examples and y will be our labels\n",
    "X = final_data.drop('is_popular', axis=1)\n",
    "y = final_data['is_popular']\n",
    "# Sanity checks\n",
    "print(\"Number of entries in actual data: \" + str(len(X.index)))\n",
    "print(\"Number of entries in label data: \" + str(len(y.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5)\n",
    "print(\"Items in training data set : \", str(len(X_train.index)))\n",
    "print(\"Items in testing data set: \", str(len(X_test.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [\n",
    "          {'title':\"Logistic regression\", 'model':LogisticRegression(solver='lbfgs', random_state=3), 'feature_set':'all'},\n",
    "          {'title':\"Logistic regression balanced weights\", 'model':LogisticRegression(solver='lbfgs',class_weight='balanced', random_state=3), 'feature_set':['energy', 'tempo', 'instrumentalness', 'danceability', 'loudness', 'valence']},\n",
    "          {'title':\"Oversampling logistic regression\", 'model':make_pipeline_imb(SMOTE(random_state=4), LogisticRegression(solver='lbfgs',random_state=3)), 'feature_set':['energy', 'danceability', 'loudness', 'valence']},\n",
    "          {'title':\"Oversampling logistic regression balanced weights\", 'model':make_pipeline_imb(SMOTE(random_state=4), LogisticRegression(solver='lbfgs',class_weight='balanced',random_state=3)), 'feature_set':'all'},\n",
    "          {'title':\"KNN\", 'model':KNeighborsClassifier(n_neighbors = 17), 'feature_set':'all'},\n",
    "          {'title':\"Oversampling KNN\", 'model':make_pipeline_imb(SMOTE(random_state=4), KNeighborsClassifier(n_neighbors = 17)), 'feature_set':['energy', 'liveness', 'acousticness', 'instrumentalness', 'danceability', 'loudness', 'valence']},\n",
    "          {'title':\"SVM\", 'model':svm.SVC(probability=True, gamma='scale', random_state=3), 'feature_set':'all'},\n",
    "          {'title':\"SVM balanced weights\", 'model':svm.SVC(probability=True, gamma='scale', random_state=3, class_weight='balanced'), 'feature_set':['energy', 'tempo', 'acousticness', 'danceability', 'loudness', 'valence']},\n",
    "          {'title':\"Oversampling SVM\", 'model':make_pipeline_imb(SMOTE(random_state=4), svm.SVC(probability=True, gamma='scale', random_state=3)), 'feature_set':['energy', 'tempo', 'speechiness', 'loudness', 'valence']},\n",
    "          {'title':\"Multilayer Perceptron\", 'model':MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=3), 'feature_set':['energy', 'tempo', 'key', 'duration', 'loudness', 'valence']},\n",
    "          {'title':\"Oversampling Multilayer Perceptron\", 'model':make_pipeline_imb(SMOTE(random_state=4), MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=3)), 'feature_set':['energy', 'tempo', 'time_signature', 'duration', 'loudness', 'valence']},\n",
    "          {'title':\"Random Forest Classifier balanced weights\", 'model':RandomForestClassifier(n_estimators=100, max_depth=2, random_state=3, class_weight=\"balanced\"), 'feature_set':'all'}\n",
    "         ]\n",
    "stats_columns = [\"Model title\", \"Accuracy\", \"Specificity\", \"Recall(Sensitivity)\", \"Precision\", \"F1\", \"AUC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "stats_columns = [\"Model title\", \"Accuracy\", \"Specificity\", \"Recall(Sensitivity)\", \"Precision\", \"F1\", \"AUC\", \"Generalized AUC\"]\n",
    "#X = np.array(X)\n",
    "#y = np.array(y)\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "cv_stats = []\n",
    "X_fs, X_test, y_fs, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "#print(X_fs.head())\n",
    "\n",
    "for item in models:\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "    print(item['title'])\n",
    "    classifier = item['model']\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    accs = []\n",
    "    specificities = []\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f1s = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    i = 0\n",
    "    cur_feature_set=item['feature_set']\n",
    "    if(cur_feature_set == 'all'):\n",
    "        X_cfs=np.array(X_fs)\n",
    "        cur_feature_set= COLUMNS_TO_SCALE\n",
    "    else:\n",
    "        X_cfs=np.array(X_fs[cur_feature_set])\n",
    "    y_fs = np.array(y_fs)\n",
    "#     print(X_cfs.head())\n",
    "    for train, test in cv.split(X_cfs, y_fs):\n",
    "        probas_ = classifier.fit(X_cfs[train], y_fs[train]).predict_proba(X_cfs[test])\n",
    "        predicts = classifier.predict(X_cfs[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_fs[test], probas_[:, 1])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        conf_matrix = confusion_matrix(y_fs[test], predicts)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        aucs.append(roc_auc)\n",
    "        accs.append(accuracy_score(y_fs[test], predicts))\n",
    "        specificities.append(float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1]))\n",
    "        recalls.append(recall_score(y_fs[test], predicts))\n",
    "        precisions.append(precision_score(y_fs[test], predicts))\n",
    "        f1s.append(f1_score(y_fs[test], predicts, average='weighted'))\n",
    "        \n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "        i += 1\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "             label='Chance', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                     label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC for ' + item['title'])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Accuracy mean: \",np.mean(accs))\n",
    "    print()\n",
    "    item['model'].fit(X_cfs, y_fs)\n",
    "    y_pred_prob = item['model'].predict_proba(X_test[cur_feature_set])[:,1]\n",
    "    test_after_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    newRow = [item['title'],np.mean(accs), np.mean(specificities), np.mean(recalls), np.mean(precisions), np.mean(f1s), mean_auc, test_after_auc]\n",
    "    cv_stats.append(newRow)\n",
    "stats_df = pd.DataFrame(cv_stats, columns = stats_columns)\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is from http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=10, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\n",
    "#           {'title':\"Logistic regression\", 'model':LogisticRegression(random_state=3), 'feature_set':['liveness', 'tempo', 'acousticness', 'danceability', 'key', 'duration']},\n",
    "#           {'title':\"Logistic regression balanced weights\", 'model':LogisticRegression(class_weight='balanced', random_state=3), 'feature_set':['tempo', 'acousticness', 'danceability', 'valence']},\n",
    "#           {'title':\"Oversampling logistic regression\", 'model':make_pipeline_imb(SMOTE(random_state=4), LogisticRegression(random_state=3)), 'feature_set':['tempo', 'acousticness', 'danceability', 'key', 'valence']},\n",
    "#           {'title':\"Oversampling logistic regression balanced weights\", 'model':make_pipeline_imb(SMOTE(random_state=4), LogisticRegression(class_weight='balanced',random_state=3)), 'feature_set':['tempo', 'acousticness', 'danceability', 'key', 'valence']},\n",
    "#           {'title':\"KNN\", 'model':KNeighborsClassifier(n_neighbors = 17), 'feature_set':['liveness', 'tempo', 'acousticness', 'danceability']},\n",
    "#           {'title':\"Oversampling KNN\", 'model':make_pipeline_imb(SMOTE(random_state=4), KNeighborsClassifier(n_neighbors = 17)), 'feature_set':['energy', 'instrumentalness', 'duration', 'valence', 'mode']},\n",
    "#           {'title':\"SVM\", 'model':svm.SVC(probability=True, gamma='scale', random_state=3), 'feature_set':['liveness', 'tempo', 'acousticness', 'danceability', 'loudness', 'mode']},\n",
    "#           {'title':\"SVM balanced weights\", 'model':svm.SVC(probability=True, gamma='scale', random_state=3, class_weight='balanced'), 'feature_set':['speechiness', 'key']},\n",
    "#           {'title':\"Oversampling SVM\", 'model':make_pipeline_imb(SMOTE(random_state=4), svm.SVC(probability=True, gamma='scale', random_state=3)), 'feature_set':['tempo', 'time_signature']},\n",
    "#           {'title':\"Multilayer Perceptron\", 'model':MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=3), 'feature_set':['energy', 'acousticness', 'danceability', 'duration', 'valence', 'mode']},\n",
    "#           {'title':\"Oversampling Multilayer Perceptron\", 'model':make_pipeline_imb(SMOTE(random_state=4), MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=3)), 'feature_set':['loudness', 'speechiness']},\n",
    "#           {'title':\"Random Forest Classifier balanced weights\", 'model':RandomForestClassifier(n_estimators=100, max_depth=2, random_state=3, class_weight=\"balanced\"), 'feature_set':['loudness', 'speechiness']}\n",
    "#          ]\n",
    "\n",
    "models1 = { \n",
    "    'Logistic regression': LogisticRegression(class_weight='balanced', random_state=3),\n",
    "    'SVM balanced weights': svm.SVC(gamma='scale', random_state=3, class_weight='balanced'),\n",
    "    'Multilayer Perceptron': MLPClassifier(random_state=3)\n",
    "}\n",
    "\n",
    "params1 = { \n",
    "    'Logistic regression': { 'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'C' : [0.001, 0.0001, 0.0001, 0.01, 0.1, 1]},\n",
    "    'SVM balanced weights': { 'kernel' : ['rbf', 'linear', 'poly', 'sigmoid'], 'gamma' : ['auto', 'scale'], 'decision_function_shape' : ['ovo', 'ovr'], 'C' : [0.001, 0.0001, 0.0001, 0.01, 0.1, 1]},\n",
    "    'Multilayer Perceptron': { 'activation' : ['identity', 'logistic', 'tanh', 'relu'], 'solver' : ['lbfgs', 'sgd', 'adam'], 'alpha' : [1e-5, 1e-4, 1, 1e-3, 1e-2], 'learning_rate' : ['constant', 'invscaling', 'adaptive']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "helper1.fit(X_fs, y_fs, scoring='roc_auc', n_jobs=10)\n",
    "helper1.score_summary(sort_by='mean_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
