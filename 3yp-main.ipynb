{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports and API setups<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function    # (at top of module)\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import json\n",
    "import spotipy\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import math\n",
    "import seaborn as sns\n",
    "import config\n",
    "\n",
    "\n",
    "# Spotify API Setup\n",
    "client_credentials_manager = SpotifyClientCredentials(config.client_id, config.client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "# Enables verbose requests tracing\n",
    "sp.trace=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in original data: 441\n"
     ]
    }
   ],
   "source": [
    "# Read the data from the file\n",
    "data = pd.read_csv('file_path_big.csv')\n",
    "data.head()\n",
    "print(\"Number of entries in original data: \" + str(len(data.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in original data after cleaning: 441\n"
     ]
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset=['Song Title', 'Artist'], keep='first')\n",
    "print(\"Number of entries in original data after cleaning: \" + str(len(data.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Mode\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Time Signature\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Key\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Popularity'].plot.hist(bins=30, rwidth=10,figsize=(10,8))\n",
    "plt.xlabel('Popularity')\n",
    "plt.title(\"Popularity histogram\")\n",
    "plt.grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.distplot(data[\"Popularity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the data according to threshold for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data according to a threshold for nicer plotting\n",
    "labeled_data = data.copy()\n",
    "threshold = 70\n",
    "labels = []\n",
    "\n",
    "# The threshold is on Popularity, see above for description of that feature\n",
    "for item in data['Popularity']:\n",
    "    if item > threshold:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "labeled_data['Is_Popular'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Is_Popular\", hue = \"Mode\", data=labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Popularity\", y = \"Duration in ms\" , hue = \"Is_Popular\", data=labeled_data).set_title(\"Duration and popularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Popularity\", y = \"Instrumentalness\" , hue = \"Is_Popular\", data=labeled_data).set_title(\"Instrumentalness and popularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Popularity\", y = \"Loudness\" , hue = \"Is_Popular\", data=labeled_data).set_title(\"Loudness and popularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any null items in our data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting popularity threshold and adding lables to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of popular examples after thresholding :  41\n",
      "Number of not popular examples after thresholding :  400\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the data to which we will ad labels and then remove any \n",
    "# columns that we will not need\n",
    "# This is currently a duplicate of the functionality above - could maybe only do this in one place\n",
    "\n",
    "final_data = data.copy()\n",
    "threshold = 92\n",
    "labels = []\n",
    "labeled_popular = 0\n",
    "labeled_notpopular = 0\n",
    "for item in data['Popularity']:\n",
    "    if item > threshold:\n",
    "        labels.append(1)\n",
    "        labeled_popular = labeled_popular + 1\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        labeled_notpopular = labeled_notpopular + 1\n",
    "final_data['Is_Popular'] = labels\n",
    "\n",
    "print('Number of popular examples after thresholding : ', labeled_popular)\n",
    "print('Number of not popular examples after thresholding : ', labeled_notpopular)\n",
    "\n",
    "# Drop unnecessary columns from original data\n",
    "final_data.drop(['Song Title', 'Artist', 'Popularity'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in actual data: 441\n",
      "Number of entries in label data: 441\n"
     ]
    }
   ],
   "source": [
    "# X will be our examples and y will be our labels\n",
    "X = final_data.drop('Is_Popular', axis=1)\n",
    "y = final_data['Is_Popular']\n",
    "# Sanity checks\n",
    "print(\"Number of entries in actual data: \" + str(len(X.index)))\n",
    "print(\"Number of entries in label data: \" + str(len(y.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in training data set :  220\n",
      "Items in testing data set:  221\n"
     ]
    }
   ],
   "source": [
    "# We split the data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "print(\"Items in training data set : \", str(len(X_train.index)))\n",
    "print(\"Items in testing data set: \", str(len(X_test.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90497737556561086"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "print(logmodel)\n",
    "# Train the model\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# Classifiy test examples\n",
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "# Print the accuracy score of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       200\n",
      "           1       0.00      0.00      0.00        21\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       221\n",
      "   macro avg       0.45      0.50      0.48       221\n",
      "weighted avg       0.82      0.90      0.86       221\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report of the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200   0]\n",
      " [ 21   0]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90497737556561086"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "nbrs = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "nbrs.fit(X_train, y_train)\n",
    "\n",
    "# Classifiy test examples\n",
    "predictionsKNN = nbrs.predict(X_test)\n",
    "accuracy_score(y_test, predictionsKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       200\n",
      "           1       0.00      0.00      0.00        21\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       221\n",
      "   macro avg       0.45      0.50      0.48       221\n",
      "weighted avg       0.82      0.90      0.86       221\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictionsKNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200,   0],\n",
       "       [ 21,   0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictionsKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
