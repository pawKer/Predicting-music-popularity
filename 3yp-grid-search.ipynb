{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function    # (at top of module)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import json\n",
    "import spotipy\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "import seaborn as sns\n",
    "import io, os, sys, types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in original data: 570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_title</th>\n",
       "      <th>artist</th>\n",
       "      <th>popularity</th>\n",
       "      <th>energy</th>\n",
       "      <th>liveness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>danceability</th>\n",
       "      <th>key</th>\n",
       "      <th>duration</th>\n",
       "      <th>loudness</th>\n",
       "      <th>valence</th>\n",
       "      <th>mode</th>\n",
       "      <th>youtube_view_count</th>\n",
       "      <th>youtube_video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spotify:track:5ygDXis42ncn6kYG14lEVG</td>\n",
       "      <td>Baby Shark</td>\n",
       "      <td>[Pinkfong]</td>\n",
       "      <td>77</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>115.062</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.825</td>\n",
       "      <td>7</td>\n",
       "      <td>96333</td>\n",
       "      <td>-3.651</td>\n",
       "      <td>0.520</td>\n",
       "      <td>1</td>\n",
       "      <td>1956582159</td>\n",
       "      <td>Baby Shark Dance | Sing and Dance! | Animal So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spotify:track:7fa9MBXhVfQ8P8Df9OEbD8</td>\n",
       "      <td>Girls Like You (feat. Cardi B)</td>\n",
       "      <td>[Maroon 5, Cardi B]</td>\n",
       "      <td>86</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>124.959</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.5680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0</td>\n",
       "      <td>235545</td>\n",
       "      <td>-6.825</td>\n",
       "      <td>0.448</td>\n",
       "      <td>1</td>\n",
       "      <td>1300452389</td>\n",
       "      <td>Maroon 5 - Girls Like You ft. Cardi B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spotify:track:6De0lHrwBfPfrhorm9q1Xl</td>\n",
       "      <td>Me Rehúso</td>\n",
       "      <td>[Danny Ocean]</td>\n",
       "      <td>83</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>104.823</td>\n",
       "      <td>0.0677</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.744</td>\n",
       "      <td>1</td>\n",
       "      <td>205715</td>\n",
       "      <td>-6.327</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1</td>\n",
       "      <td>1229501096</td>\n",
       "      <td>Danny Ocean -  Me Rehúso (Official Audio)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spotify:track:1j6xOGusnyXq3l6IryKF3G</td>\n",
       "      <td>Déjala Que Vuelva (feat. Manuel Turizo)</td>\n",
       "      <td>[Piso 21, Manuel Turizo]</td>\n",
       "      <td>74</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>170.019</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.681</td>\n",
       "      <td>1</td>\n",
       "      <td>220117</td>\n",
       "      <td>-4.323</td>\n",
       "      <td>0.839</td>\n",
       "      <td>1</td>\n",
       "      <td>1216075058</td>\n",
       "      <td>Piso 21 - Déjala Que Vuelva (feat. Manuel Turi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spotify:track:2ijef6ni2amuunRoKTlgww</td>\n",
       "      <td>Sin Pijama</td>\n",
       "      <td>[Becky G, Natti Natasha]</td>\n",
       "      <td>90</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>94.014</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>4</td>\n",
       "      <td>0.791</td>\n",
       "      <td>11</td>\n",
       "      <td>188560</td>\n",
       "      <td>-3.695</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0</td>\n",
       "      <td>1071141995</td>\n",
       "      <td>Becky G Natti Natasha - Sin Pijama (Video Ofic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                song_id  \\\n",
       "0  spotify:track:5ygDXis42ncn6kYG14lEVG   \n",
       "1  spotify:track:7fa9MBXhVfQ8P8Df9OEbD8   \n",
       "2  spotify:track:6De0lHrwBfPfrhorm9q1Xl   \n",
       "3  spotify:track:1j6xOGusnyXq3l6IryKF3G   \n",
       "4  spotify:track:2ijef6ni2amuunRoKTlgww   \n",
       "\n",
       "                                song_title                    artist  \\\n",
       "0                               Baby Shark                [Pinkfong]   \n",
       "1           Girls Like You (feat. Cardi B)       [Maroon 5, Cardi B]   \n",
       "2                                Me Rehúso             [Danny Ocean]   \n",
       "3  Déjala Que Vuelva (feat. Manuel Turizo)  [Piso 21, Manuel Turizo]   \n",
       "4                               Sin Pijama  [Becky G, Natti Natasha]   \n",
       "\n",
       "   popularity  energy  liveness    tempo  speechiness  acousticness  \\\n",
       "0          77   0.840    0.3410  115.062       0.2270        0.2450   \n",
       "1          86   0.541    0.1300  124.959       0.0505        0.5680   \n",
       "2          83   0.804    0.0494  104.823       0.0677        0.0231   \n",
       "3          74   0.788    0.0753  170.019       0.0785        0.0482   \n",
       "4          90   0.745    0.1040   94.014       0.0464        0.3540   \n",
       "\n",
       "   instrumentalness  time_signature  danceability  key  duration  loudness  \\\n",
       "0          0.000000               4         0.825    7     96333    -3.651   \n",
       "1          0.000000               4         0.851    0    235545    -6.825   \n",
       "2          0.000000               4         0.744    1    205715    -6.327   \n",
       "3          0.000000               4         0.681    1    220117    -4.323   \n",
       "4          0.000029               4         0.791   11    188560    -3.695   \n",
       "\n",
       "   valence  mode  youtube_view_count  \\\n",
       "0    0.520     1          1956582159   \n",
       "1    0.448     1          1300452389   \n",
       "2    0.426     1          1229501096   \n",
       "3    0.839     1          1216075058   \n",
       "4    0.820     0          1071141995   \n",
       "\n",
       "                                 youtube_video_title  \n",
       "0  Baby Shark Dance | Sing and Dance! | Animal So...  \n",
       "1              Maroon 5 - Girls Like You ft. Cardi B  \n",
       "2          Danny Ocean -  Me Rehúso (Official Audio)  \n",
       "3  Piso 21 - Déjala Que Vuelva (feat. Manuel Turi...  \n",
       "4  Becky G Natti Natasha - Sin Pijama (Video Ofic...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from the file\n",
    "data = pd.read_csv('Data/data_500_entries_youtube.csv')\n",
    "# data = pd.read_csv('Data/data_3000_entries_youtube.csv')\n",
    "\n",
    "print(\"Number of entries in original data: \" + str(len(data.index)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of popular examples after thresholding :  59\n",
      "Number of not popular examples after thresholding :  511\n"
     ]
    }
   ],
   "source": [
    "from project_modules import *\n",
    "\n",
    "# Depending on the dataset we have different ways of labeling the data\n",
    "if 'total_no_streams' in data.columns:\n",
    "    # In the 3000 song dataset there are 3 features that can be used for \n",
    "    # labeling : popularity, total_no_streams, youtube_view_count\n",
    "    data = data[data.popularity > 10]\n",
    "    data = data[data.total_no_streams > 999999]\n",
    "    data = data[data.youtube_view_count > 999999]\n",
    "    final_data = label_data_combined(data, 89, 700000000, 700000000)\n",
    "else:\n",
    "    # In the 500 song dataset, there are 2 features that can be used for\n",
    "    # labeling : popularity and youtube_view_count\n",
    "    final_data = label_data_yt(data, 89, 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary text columns or columns used for labeling from data\n",
    "if 'total_no_streams' in data.columns:\n",
    "    final_data.drop(['song_id', 'song_title', 'artist', 'popularity', 'youtube_view_count', 'youtube_video_title', 'total_no_streams'], 1, inplace=True)\n",
    "else:\n",
    "    final_data.drop(['song_id', 'song_title', 'artist', 'popularity', 'youtube_view_count', 'youtube_video_title'], 1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in actual data: 570\n",
      "Number of entries in label data: 570\n"
     ]
    }
   ],
   "source": [
    "# X will be our examples and y will be our labels\n",
    "X = final_data.drop('is_popular', axis=1)\n",
    "y = final_data['is_popular']\n",
    "\n",
    "# Sanity checks\n",
    "print(\"Number of entries in actual data: \" + str(len(X.index)))\n",
    "print(\"Number of entries in label data: \" + str(len(y.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is from http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=10, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "models1 = { \n",
    "    \"KNN\":KNeighborsClassifier(),\n",
    "    'Logistic regression': LogisticRegression(class_weight='balanced', random_state=3),\n",
    "    'SVM balanced weights': svm.SVC(class_weight='balanced'),\n",
    "    'Multilayer Perceptron': make_pipeline_imb(SMOTE(random_state=5), MLPClassifier(activation=activ, solver=solv, alpha=al, learning_rate=lrn, hidden_layer_sizes=hdn, random_state=3)),\n",
    "    'Random Forest Classifier': RandomForestClassifier(n_estimators=100, max_depth=2, random_state=3, class_weight=\"balanced\")\n",
    "}\n",
    "\n",
    "params1 = { \n",
    "     'KNN':{'n_neighbors':[1,3,5,7,9,11,13,15,17, 19, 21]},\n",
    "     'Logistic regression': { 'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "                              'C' : [0.001, 0.0001, 0.0001, 0.01, 0.1, 1]},\n",
    "     'SVM balanced weights': { 'kernel' : ['rbf', 'linear', 'poly', 'sigmoid'], \n",
    "                               'gamma' : ['auto', 'scale'], \n",
    "                               'decision_function_shape' : ['ovo', 'ovr'],\n",
    "                               'C' : [0.001, 0.0001, 0.0001, 0.01, 0.1, 1]},\n",
    "    'Multilayer Perceptron': { 'activation' : ['identity', 'logistic', 'tanh', 'relu'],                                 'solver' : ['lbfgs', 'sgd', 'adam'], \n",
    "                               'alpha' : [1e-5, 1e-4, 1, 1e-3, 1e-2, 1e-1], \n",
    "                               'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "                               'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],},\n",
    "     'Random Forest Classifier':{ 'n_estimators' : [10, 100, 200, 500, 1000], \n",
    "                                  'max_depth': [2, 5, 7, 10, 13], \n",
    "                                  'criterion' : ['gini', 'entropy']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler2 = preprocessing.StandardScaler()\n",
    "scaled_X = pd.DataFrame(scaler2.fit_transform(X))\n",
    "\n",
    "# Initialize Grid Search class\n",
    "helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "# Run Grid Search\n",
    "helper1.fit(scaled_X, y, scoring='roc_auc', n_jobs=10, refit=True)\n",
    "# Print summary sorted by mean score\n",
    "helper1.score_summary(sort_by='mean_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper1.score_summary(sort_by='mean_score').to_csv(\"scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search for imbalanced pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for SMOTE pipelines\n",
    "# This is some code I implemented for imblearn pipelines because\n",
    "# they don't work with the algorithm above\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scaler2 = preprocessing.StandardScaler()\n",
    "scaled_X = pd.DataFrame(scaler2.fit_transform(X))\n",
    "print(params1)\n",
    "scores_array = []\n",
    "param_columns = [\"activation\", \"solver\", \"alpha\", \"learning_rate\", \"hidden_layer_sizes, score\"]\n",
    "\n",
    "for activ in params1['Multilayer Perceptron']['activation']:\n",
    "    for solv in params1['Multilayer Perceptron']['solver']:\n",
    "        for al in params1['Multilayer Perceptron']['alpha']:\n",
    "            for lrn in params1['Multilayer Perceptron']['learning_rate']:\n",
    "                for hdn in params1['Multilayer Perceptron']['hidden_layer_sizes']:\n",
    "                    mlp_smote = make_pipeline_imb(SMOTE(random_state=5), MLPClassifier(activation=activ, solver=solv, alpha=al, learning_rate=lrn, hidden_layer_sizes=hdn, random_state=3))\n",
    "                    score = cross_val_score(mlp_smote, scaled_X, y, cv=StratifiedShuffleSplit(n_splits=10), scoring='roc_auc')\n",
    "                    scores_array.append([activ, solv, al, lrn, hdn, score])\n",
    "scores_grid = pd.DataFrame(scores_array, columns=param_columns)\n",
    "scores_grid.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array=[]\n",
    "for i in range(0, len(scores_array), 1):\n",
    "    new_array.append([scores_array[i][0], scores_array[i][1], scores_array[i][2], scores_array[i][3], scores_array[i][4], scores_array[i][5].mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_columns = [\"index\",\"activation\", \"solver\", \"alpha\", \"learning_rate\", \"hidden_layer_sizes, score\"]\n",
    "scores_grid = pd.DataFrame(new_array, columns=param_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_grid.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_grid.to_csv(\"mlp_smote_scores.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
