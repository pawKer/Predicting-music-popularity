{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports and API setups<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function    # (at top of module)\n",
    "import warnings\n",
    "#warnings.filterwarnings('always')\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import json\n",
    "import spotipy\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "import seaborn as sns\n",
    "import config\n",
    "\n",
    "\n",
    "# Spotify API Setup\n",
    "client_credentials_manager = SpotifyClientCredentials(config.client_id, config.client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "# Enables verbose requests tracing\n",
    "sp.trace=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the file\n",
    "data = pd.read_csv('data_600_entries.csv')\n",
    "print(\"Number of entries in original data: \" + str(len(data.index)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'song_id' in data.columns:\n",
    "    data = data.drop_duplicates(subset=['song_id'], keep='first')\n",
    "else:\n",
    "    data = data.drop_duplicates(subset=['song_title'], keep='first')\n",
    "    \n",
    "print(\"Number of entries in original data after cleaning: \" + str(len(data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.popularity > 50]\n",
    "print(\"Number of entries in original data after cleaning: \" + str(len(data.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any null items in our data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting popularity threshold and adding lables to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data to which we will ad labels and then remove any \n",
    "# columns that we will not need\n",
    "# This is currently a duplicate of the functionality above - could maybe only do this in one place\n",
    "\n",
    "final_data = data.copy()\n",
    "threshold = 90\n",
    "labels = []\n",
    "labeled_popular = 0\n",
    "labeled_notpopular = 0\n",
    "for item in data['popularity']:\n",
    "    if item > threshold:\n",
    "        labels.append(1)\n",
    "        labeled_popular = labeled_popular + 1\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        labeled_notpopular = labeled_notpopular + 1\n",
    "final_data['is_popular'] = labels\n",
    "\n",
    "print('Number of popular examples after thresholding : ', labeled_popular)\n",
    "print('Number of not popular examples after thresholding : ', labeled_notpopular)\n",
    "\n",
    "# Drop unnecessary columns from original data\n",
    "if 'song_id' in data.columns:\n",
    "    final_data.drop(['song_id', 'song_title', 'artist', 'popularity'], 1, inplace=True)\n",
    "else:\n",
    "    final_data.drop(['song_title', 'artist', 'popularity'], 1, inplace=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "\n",
    "COLUMNS_TO_SCALE = [\"energy\", \"liveness\", \"tempo\", \n",
    "                    \"speechiness\", \"acousticness\", \"instrumentalness\", \n",
    "                    \"time_signature\", \"danceability\", \"key\", \n",
    "                    \"duration\", \"loudness\", \"valence\", \"mode\"]\n",
    "\n",
    "# Keep data in a temp variable for testing\n",
    "scaled_data = final_data.copy()\n",
    "\n",
    "# Normalization\n",
    "#scaled_data[COLUMNS_TO_SCALE] = MinMaxScaler().fit_transform(scaled_data[COLUMNS_TO_SCALE])\n",
    "\n",
    "#Standardization\n",
    "scaled_data[COLUMNS_TO_SCALE] = scale(scaled_data[COLUMNS_TO_SCALE])\n",
    "\n",
    "# Plots to see the difference before/after scaling\n",
    "plt.plot(final_data['duration'])\n",
    "plt.title(\"Before scaling\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(scaled_data['duration'])\n",
    "plt.title(\"After scaling\")\n",
    "plt.show()\n",
    "\n",
    "# Copy data back\n",
    "final_data = scaled_data.copy()\n",
    "\n",
    "#Just to check that everything is fine\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract labels from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X will be our examples and y will be our labels\n",
    "X = final_data.drop('is_popular', axis=1)\n",
    "y = final_data['is_popular']\n",
    "# Sanity checks\n",
    "print(\"Number of entries in actual data: \" + str(len(X.index)))\n",
    "print(\"Number of entries in label data: \" + str(len(y.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5)\n",
    "print(\"Items in training data set : \", str(len(X_train.index)))\n",
    "print(\"Items in testing data set: \", str(len(X_test.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Can use the class_weight argument to assign different weights to the classes\n",
    "# balanced - calculates the right weights according to the number of examples\n",
    "# each class\n",
    "\n",
    "logmodel = LogisticRegression(class_weight='balanced',random_state=3)\n",
    "#logmodel = LogisticRegression(random_state=3)\n",
    "\n",
    "print(logmodel)\n",
    "# Train the model\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# Classifiy test examples\n",
    "predictions = logmodel.predict(X_test)\n",
    "print()\n",
    "\n",
    "# Print the accuracy score of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy: \",accuracy_score(y_test, predictions))\n",
    "\n",
    "print()\n",
    "\n",
    "# Print the classification report of the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Print the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "# ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "y_pred_prob = logmodel.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,0],[1,1])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "plt.title('ROC curve for Logistic Regression without 10CV')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# AUC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"AUC score: \", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "classifier = LogisticRegression(class_weight='balanced',random_state=3)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "tprs = []\n",
    "aucs = []\n",
    "accs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    predicts = classifier.predict(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    accs.append(accuracy_score(y[test], predicts))\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Balanced Logistic Regression with 10CV')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy mean: \",np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "nbrs = KNeighborsClassifier(n_neighbors = 14)\n",
    "\n",
    "nbrs.fit(X_train, y_train)\n",
    "\n",
    "# Classifiy test examples\n",
    "predictionsKNN = nbrs.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictionsKNN))\n",
    "print()\n",
    "print(classification_report(y_test, predictionsKNN))\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predictionsKNN))\n",
    "\n",
    "# ROC curve\n",
    "y_pred_prob_knn = nbrs.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_knn)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "plt.title('ROC curve for KNN')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# AUC score\n",
    "print(\"AUC score: \", roc_auc_score(y_test, y_pred_prob_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for the best k value using the cross validation score\n",
    "k_range = list(range(1, 25))\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "best_value = max(k_scores)\n",
    "best_value_index = 0\n",
    "index = 1\n",
    "for value in k_scores:\n",
    "    if value == best_value:\n",
    "        best_value_index = index\n",
    "        break\n",
    "    index = index + 1\n",
    "    \n",
    "print(k_scores)\n",
    "print(\"Best K: \", best_value_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "classifier = KNeighborsClassifier(n_neighbors=14)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "tprs = []\n",
    "aucs = []\n",
    "accs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    preds = classifier.predict(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    accs.append(accuracy_score(y[test], preds))\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"Accuracy mean: \",np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will use over-sampling with SMOTE(Synthetic Minority Oversampling Technique) to make up for the lack of examples of popular songs. This is one approach to fix the imbalanced class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over-sampling with SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "classifier = LogisticRegression(random_state=3)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(random_state=4), classifier)\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "predictionsLogImb = smote_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to showcase the effect of oversampling on our data\n",
    "from collections import Counter\n",
    "X_smote, y_smote = SMOTE(random_state=4).fit_sample(X, y)\n",
    "print(\"Normal data distribution: \", Counter(y))\n",
    "print(\"SMOTE data distribution: \", Counter(y_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \",accuracy_score(y_test, predictionsLogImb))\n",
    "print()\n",
    "print(classification_report_imbalanced(y_test, predictionsLogImb))\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predictionsLogImb))\n",
    "\n",
    "# ROC curve\n",
    "y_pred_prob_log_imb = smote_model.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_log_imb)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "plt.title('ROC curve for Logistic Regression with SMOTE')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# AUC score\n",
    "print(\"AUC score: \", roc_auc_score(y_test, y_pred_prob_log_imb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "classifier = LogisticRegression(random_state=3)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "tprs = []\n",
    "aucs = []\n",
    "accs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    pipeline = make_pipeline_imb(SMOTE(random_state=4), classifier)\n",
    "    model = pipeline.fit(X[train], y[train])\n",
    "    probas_ = model.predict_proba(X[test])\n",
    "    preds = model.predict(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    accs.append(accuracy_score(y[test], preds))\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for Logistic Regression with SMOTE')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"Accuracy mean: \",np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierKNN = KNeighborsClassifier(n_neighbors = 14)\n",
    "smote_pipeline_knn = make_pipeline_imb(SMOTE(), classifierKNN)\n",
    "smote_model_knn = smote_pipeline_knn.fit(X_train, y_train)\n",
    "predictionsKnnImb = smote_model_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_test, predictionsKnnImb))\n",
    "print()\n",
    "print(classification_report_imbalanced(y_test, predictionsKnnImb))\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predictionsKnnImb))\n",
    "\n",
    "# ROC curve\n",
    "y_pred_prob_knn_imb = smote_model_knn.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_knn_imb)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "plt.title('ROC curve for KNN with SMOTE')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# AUC score\n",
    "print(\"AUC score: \", roc_auc_score(y_test, y_pred_prob_knn_imb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "classifier = KNeighborsClassifier(n_neighbors=14)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "tprs = []\n",
    "aucs = []\n",
    "accs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    pipeline = make_pipeline_imb(SMOTE(random_state=4), classifier)\n",
    "    model = pipeline.fit(X[train], y[train])\n",
    "    probas_ = model.predict_proba(X[test])\n",
    "    preds = model.predict(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    accs.append(accuracy_score(y[test], preds))\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for KNN with SMOTE')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"Accuracy mean: \",np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
